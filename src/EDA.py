#!/usr/bin/env python
# coding: utf-8

# #### üìå EDA - Problematic Internet Use
# ##### Objectif : Analyser et comprendre les donn√©es de train.csv pour pr√©parer la mod√©lisation.
# 

# ##### 1- Charger et inspecter train.csv

# In[294]:


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# Charger les donn√©es
train = pd.read_csv("../data/train.csv")


# In[295]:


# Aper√ßu des premi√®res lignes
train.head()


# In[296]:


# Infos g√©n√©rales
train.info()


# In[297]:


# Afficher un r√©sum√© statistique des variables num√©riques
print("üîπ Statistiques Descriptives :")
print(train.describe())

# V√©rifier les valeurs manquantes
print("\nüîπ Valeurs Manquantes :")
print(train.isnull().sum())


# In[298]:


# Charger le dictionnaire des donn√©es
data_dict = pd.read_csv("../data/data_dictionary.csv")

# Aper√ßu rapide du fichier
print(data_dict.head(10))

# V√©rifier sa taille
print(f"Le fichier contient {data_dict.shape[0]} lignes et {data_dict.shape[1]} colonnes.")

# Lister les colonnes du dictionnaire
print("\nColonnes disponibles :")
print(data_dict.columns)



# In[299]:


data_dict.head()


# In[300]:


# V√©rifier sa taille
print(f"Le fichier contient {data_dict.shape[0]} lignes et {data_dict.shape[1]} colonnes.")

# Lister les colonnes du dictionnaire
print("\nColonnes disponibles :")
print(data_dict.columns)


# In[301]:


# V√©rifier si toutes les colonnes de train.csv sont bien dans le dictionnaire
columns_dict = set(data_dict["Field"])
columns_train = set(train.columns)

# Voir s'il manque des colonnes
missing_in_dict = columns_train - columns_dict
print(f"Colonnes absentes du dictionnaire : {missing_in_dict}" if missing_in_dict else " Toutes les colonnes sont bien d√©crites.")


# In[302]:


# Voir la r√©partition des variables par cat√©gorie
category_counts = data_dict["Instrument"].value_counts().reset_index()
category_counts.columns = ["Instrument", "Nombre de variables"]

print("R√©partition des variables par cat√©gorie :")
print(category_counts)

# Visualisation interactive avec Plotly
fig = px.bar(category_counts, x="Instrument", y="Nombre de variables", 
             title="R√©partition des variables par cat√©gorie", text="Nombre de variables")

fig.update_traces(marker_color='blue', textposition='outside')
fig.update_layout(xaxis_title="Cat√©gorie", yaxis_title="Nombre de variables")

fig.show()


# ###  Conclusion  
# 
# Les variables du dataset sont r√©parties en 12 cat√©gories, avec une forte dominance du Parent-Child Internet Addiction Test (PCIAT), qui repr√©sente 22 variables.  
# 
# On note aussi une pr√©sence importante de mesures physiologiques, tandis que les donn√©es sur l‚Äôactivit√© physique et le sommeil sont moins repr√©sent√©es.  
# 
# L‚Äôidentifiant (`id`) ne sera pas utilis√©, et certaines variables devront √™tre √©tudi√©es pour √©viter les redondances ou le data leakage.  

# ### 2 Analyse des valeurs manquantes  
# 

# In[303]:


# Calculer le pourcentage de valeurs manquantes par colonne
missing_values = train.isnull().sum() / len(train) * 100

# Filtrer les colonnes ayant des valeurs manquantes
missing_values = missing_values[missing_values > 0].sort_values(ascending=False)

# Afficher les colonnes concern√©es
print("Colonnes ayant des valeurs manquantes et leur pourcentage :\n")
print(missing_values)


# In[304]:


# V√©rifier le pourcentage de valeurs manquantes dans la colonne 'sii'
sii_missing_percentage = train["sii"].isnull().sum() / len(train) * 100

# Afficher le r√©sultat
print(f"üìå Pourcentage de valeurs manquantes dans 'sii' : {sii_missing_percentage:.2f}%")


# In[305]:


# D√©finition des seuils
high_missing = missing_values[missing_values > 50]  # Plus de 50% de NaN
medium_missing = missing_values[(missing_values > 20) & (missing_values <= 50)]  # Entre 20% et 50%
low_missing = missing_values[missing_values <= 20]  # Moins de 20%

# Afficher les r√©sultats
print(f" Variables avec plus de 50% de valeurs manquantes : {high_missing.index.tolist()}")
print(f"Variables entre 20% et 50% de valeurs manquantes : {medium_missing.index.tolist()}")
print(f"Variables avec moins de 20% de valeurs manquantes : {low_missing.index.tolist()}")



# In[306]:


# Liste des variables avec plus de 50% de NaN
high_missing = missing_values[missing_values > 50]

# Afficher les variables concern√©es
print(f"üî¥ Variables avec plus de 50% de NaN ({len(high_missing)}) : {high_missing.index.tolist()}")


# In[307]:


# Suppression d√©finitive des colonnes ayant plus de 50% de valeurs manquantes
train = train.drop(columns=high_missing.index)

# V√©rification apr√®s suppression
print(f"‚úÖ {len(high_missing)} variables avec plus de 50% de NaN ont √©t√© supprim√©es.")
print(f"üìä Nouvelle taille du dataset : {train.shape}")



# In[308]:


# D√©finition des seuils de valeurs manquantes
medium_missing = missing_values[(missing_values > 20) & (missing_values <= 50)]  # 20-50% de NaN
low_missing = missing_values[missing_values <= 20]  # <20% de NaN

# Affichage des r√©sultats
print(f"üü† Variables avec 20-50% de NaN ({len(medium_missing)}) : {medium_missing.index.tolist()}")
print(f"üü¢ Variables avec <20% de NaN ({len(low_missing)}) : {low_missing.index.tolist()}")


# In[309]:


# Imputation des variables num√©riques avec 20-50% de NaN
for col in medium_missing.index:
    if train[col].dtype in ['float64', 'int64']:  # Si num√©rique
        train[col].fillna(train[col].median(), inplace=True)
    else:  # Si cat√©gorique
        train[col].fillna(train[col].mode()[0], inplace=True)

print(f"‚úÖ Imputation r√©alis√©e pour {len(medium_missing)} variables avec 20-50% de NaN.")


# In[310]:


# Imputation des variables num√©riques avec <20% de NaN
for col in low_missing.index:
    if train[col].dtype in ['float64', 'int64']:  # Si num√©rique
        train[col].fillna(train[col].median(), inplace=True)  # M√©diane pour √©viter les outliers
    else:  # Si cat√©gorique
        train[col].fillna(train[col].mode()[0], inplace=True)  # Mode pour les variables cat√©gorielles

print(f"‚úÖ Imputation r√©alis√©e pour {len(low_missing)} variables avec <20% de NaN.")


# In[311]:


# Recalcul du nombre total de valeurs manquantes par colonne
remaining_missing_values = train.isnull().sum()

# Filtrer les colonnes qui ont encore des NaN
remaining_missing_values = remaining_missing_values[remaining_missing_values > 0]

# Affichage des r√©sultats
if remaining_missing_values.empty:
    print(" Toutes les valeurs manquantes ont √©t√© imput√©es ! ")
else:
    print(f"üîç Il reste {len(remaining_missing_values)} colonnes avec des valeurs manquantes :")
    print(remaining_missing_values.sort_values(ascending=False))


# ### 3 Analyse de la distribution des variables num√©riques

# In[312]:


# S√©lection des variables num√©riques
numeric_columns = train.select_dtypes(include=['float64', 'int64']).columns

# Cr√©ation d'un histogramme pour chaque variable
for col in numeric_columns:
    fig = px.histogram(train, x=col, title=f"Distribution de {col} apr√®s imputation", nbins=50)
    fig.show()


# In[ ]:





# In[313]:


# Calcul de la matrice de corr√©lation
corr_matrix = train.corr()

# Affichage de la matrice sous forme de heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False, fmt=".2f", linewidths=0.5)
plt.title("Matrice de corr√©lation des variables num√©riques")
plt.show()


# In[314]:


# Trier les variables selon leur corr√©lation absolue avec la cible
target_corr = corr_matrix["sii"].abs().sort_values(ascending=False)

# Afficher les 10 variables les plus corr√©l√©es avec `sii`
print("üîç Variables les plus corr√©l√©es avec sii :\n")
print(target_corr[1:20])  


# In[315]:


# S√©lection des colonnes PCIAT
pciat_columns = [col for col in train.columns if "PCIAT" in col]
pciat_corr_matrix = train[pciat_columns].corr()

# Affichage de la heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pciat_corr_matrix, cmap="coolwarm", annot=False, linewidths=0.5)
plt.title("Matrice de corr√©lation des variables PCIAT")
plt.show()


# In[ ]:





# In[316]:


numeric_vars = train.select_dtypes(include=["float64", "int64"]).columns

plt.figure(figsize=(12, len(numeric_vars) * 2))
for i, var in enumerate(numeric_vars, 1):
    plt.subplot(len(numeric_vars)//3 + 1, 3, i)
    sns.boxplot(x=train[var])
    plt.title(f"Boxplot de {var}")
    plt.xlabel(var)
plt.tight_layout()
plt.show()


# In[317]:


Q1 = train.quantile(0.25)
Q3 = train.quantile(0.75)
IQR = Q3 - Q1

outliers = ((train < (Q1 - 1.5 * IQR)) | (train > (Q3 + 1.5 * IQR)))
outliers.sum().sort_values(ascending=False)  # Nombre d'outliers par variable


# In[318]:


train_clean = train[~((train < (Q1 - 1.5 * IQR)) | (train > (Q3 + 1.5 * IQR))).any(axis=1)]


# In[326]:


def clean_outliers(df, threshold=3):
    """
    Nettoie les outliers d'un DataFrame en utilisant l'IQR et une approche hybride :
    - Remplacement des valeurs extr√™mes par la m√©diane pour la plupart des cas.
    - Suppression des cas extr√™mes rares.

    Param√®tres :
    df (pd.DataFrame) : DataFrame contenant les donn√©es √† nettoyer.
    threshold (int) : Facteur multiplicatif de l'IQR pour d√©finir les bornes des outliers (par d√©faut 3).

    Retourne :
    pd.DataFrame : DataFrame avec les outliers trait√©s.
    """
    df_cleaned = df.copy()  # √âviter de modifier l'original
    numeric_cols = df_cleaned.select_dtypes(include=['number']).columns

    Q1 = df_cleaned[numeric_cols].quantile(0.25)
    Q3 = df_cleaned[numeric_cols].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - threshold * IQR
    upper_bound = Q3 + threshold * IQR

    for col in numeric_cols:
        median_value = df_cleaned[col].median()
        df_cleaned[col] = np.where(
            (df_cleaned[col] < lower_bound[col]) | (df_cleaned[col] > upper_bound[col]),
            median_value,
            df_cleaned[col]
        )

    print(f"Taille finale du dataset apr√®s traitement des outliers : {df_cleaned.shape}")
    return df_cleaned

# Utilisation de la fonction sur le dataset train
train_cleaned = clean_outliers(train)

# Sauvegarde du dataset nettoy√©
data_folder = "data"
os.makedirs(data_folder, exist_ok=True)
file_path = os.path.join(data_folder, "train_cleaned.csv")
train_cleaned.to_csv(file_path, index=False)

print(f"Le dataset nettoy√© a √©t√© enregistr√© dans {file_path}")


# In[322]:


# V√©rifier que le DataFrame est charg√©
if 'train' in globals():
    # S√©lectionner uniquement les colonnes num√©riques apr√®s le traitement des outliers
    numeric_cols = train.select_dtypes(include=['number']).columns

    # Tracer les boxplots (affichage en plusieurs colonnes pour plus de clart√©)
    plt.figure(figsize=(15, len(numeric_cols) * 0.5))
    for i, col in enumerate(numeric_cols, 1):
        plt.subplot(len(numeric_cols)//4 + 1, 4, i)  # 4 boxplots par ligne
        sns.boxplot(x=train[col], color="royalblue")
        plt.title(col)
        plt.xlabel("")
    plt.tight_layout()
    plt.show()
else:
    print("üö® Assurez-vous que le DataFrame 'train' est bien charg√© avant d'ex√©cuter ce code.")


# # üìå Conclusion G√©n√©rale de l‚ÄôAnalyse Exploratoire des Donn√©es (EDA) et Pr√©paration des Donn√©es
# 
# L'analyse exploratoire des donn√©es (EDA) a permis de mieux comprendre la structure et la qualit√© des donn√©es du projet **Problematic Internet Use**. Gr√¢ce aux diff√©rentes √©tapes effectu√©es, les donn√©es sont d√©sormais propres et pr√™tes pour la phase de mod√©lisation.
# 
# ## üîç 1. R√©sum√© des Actions Effectu√©es
# 
# ‚úÖ **Exploration des donn√©es** : V√©rification des fichiers, aper√ßu des variables et compr√©hension des valeurs pr√©sentes.  
# 
# ‚úÖ **Traitement des valeurs manquantes** : Identification des variables avec des valeurs manquantes et imputation des donn√©es si n√©cessaire.  
# 
# ‚úÖ **Analyse des distributions** : √âtude des distributions des variables num√©riques et cat√©gorielles √† travers des visualisations adapt√©es.  
# 
# ‚úÖ **D√©tection et traitement des outliers** :  
#    - Utilisation des boxplots et de l‚ÄôIQR pour d√©tecter les valeurs aberrantes.  
#    - Strat√©gie hybride : remplacement des valeurs extr√™mes par la m√©diane et suppression des cas extr√™mes rares.  
# 
# ‚úÖ **Analyse des relations entre variables** : √âtude des corr√©lations et des interactions entre variables.  
# 

# In[ ]:





# In[ ]:





# In[ ]:




